# -*- coding: utf-8 -*-
"""keras_estimator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1chKcWGCvWkcCBLOR_ToUxy81-JE-raI6
"""

import os
import time
#!pip install -q -U tensorflow-gpu
import tensorflow as tf
import numpy as np


"""# 下载数据"""

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()

"""# 数据预处理"""

TRAINING_SIZE = len(train_images)
TEST_SIZE = len(test_images)
train_images = np.asarray(train_images, dtype=np.float32) / 255
# Convert the train images and add channels
train_images = train_images.reshape((TRAINING_SIZE, 28, 28, 1))
test_images = np.asarray(test_images, dtype=np.float32) / 255
# Convert the train images and add channels
test_images = test_images.reshape((TEST_SIZE, 28, 28, 1))

# How many categories we are predicting from (0-9)
LABEL_DIMENSIONS = 10
train_labels = tf.keras.utils.to_categorical(train_labels, LABEL_DIMENSIONS)
test_labels = tf.keras.utils.to_categorical(test_labels, LABEL_DIMENSIONS)
# Cast the labels to floats, needed later
train_labels = train_labels.astype(np.float32)
test_labels = test_labels.astype(np.float32)

####---------------- model build by yourself  ----------------####
# inputs = tf.keras.Input(shape=(28,28,1))  # Returns a placeholder tensor
# x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu)(inputs)
# x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)
# x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu)(x)
# x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)
# x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu)(x)
# x = tf.keras.layers.Flatten()(x)
# x = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)
# predictions = tf.keras.layers.Dense(LABEL_DIMENSIONS, activation=tf.nn.softmax)(x)

# model = tf.keras.Model(inputs=inputs, outputs=predictions)
# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
# model.compile(loss='categorical_crossentropy',
#               optimizer=optimizer,
#               metrics=['accuracy'])
####---------------- triplet model  ----------------####
inputs   = tf.keras.Input(shape=(28,28,1))
x     = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same')(inputs)
x     = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, trainable=True)(x)
x     = tf.keras.layers.ReLU()(x)
x     = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)
x     = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same')(inputs)
x     = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, trainable=True)(x)
x     = tf.keras.layers.ReLU()(x)
x     = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)
x     = tf.keras.layers.Flatten()(x)
x     = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)
prediction = tf.keras.layers.Dense(LABEL_DIMENSIONS, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=prediction)
optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

"""# **创建Estimator**"""

# NUM_GPUS = 1
# strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)
# config = tf.estimator.RunConfig(train_distribute=strategy)
# estimator = tf.keras.estimator.model_to_estimator(model, config=config)
# NUM_GPUS = 1
# strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)
# config = tf.estimator.RunConfig(train_distribute=strategy)
estimator = tf.keras.estimator.model_to_estimator(model)

"""#创建Estimator输入函数"""

def input_fn(images, labels, epochs, batch_size):
    # Convert the inputs to a Dataset. (E)
    dataset = tf.data.Dataset.from_tensor_slices((images, labels))
    # Shuffle, repeat, and batch the examples. (T)
    SHUFFLE_SIZE = 5000
    dataset = dataset.shuffle(SHUFFLE_SIZE).repeat(epochs).batch(batch_size)
    dataset = dataset.prefetch(None)
    # Return the dataset. (L)
    return dataset

"""#训练Estimator"""

class TimeHistory(tf.train.SessionRunHook):
    def begin(self):
        self.times = []

    def before_run(self, run_context):
        self.iter_time_start = time.time()

    def after_run(self, run_context, run_values):
        self.times.append(time.time() - self.iter_time_start)

BATCH_SIZE = 512
EPOCHS = 5
time_hist = TimeHistory()
# traindata = input_fn(train_images, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE)
# print(traindata.shape)

estimator.train(input_fn=lambda: input_fn(train_images, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE), hooks=[time_hist])

"""# Performance"""

total_time =  sum(time_hist.times)
# print(f"total time with {NUM_GPUS} GPUs: {total_time} seconds")
print(f"total time with GPUs: {total_time} seconds")
avg_time_per_batch = np.mean(time_hist.times)
# print(f"{BATCH_SIZE*NUM_GPUS/avg_time_per_batch} images/second with {NUM_GPUS} GPUs")

"""# Evaluate Estimator"""

estimator.evaluate(lambda:input_fn(test_images, test_labels, epochs=1, batch_size=BATCH_SIZE))